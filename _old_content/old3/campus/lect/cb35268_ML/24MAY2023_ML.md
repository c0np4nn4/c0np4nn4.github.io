+++
title = "ML 24MAY2023"
description = "Deep Learning"
date = 2023-05-24
toc = true

[taxonomies]
categories = ["ML"]
tags = ["ML"]

[extra]
math=true
+++

---

# Review: MLP
- `Back-Propagation`
  - delta, weight, loss function, etc.

---

# Convolutional Neural Network (CNN)
- MLP 에서는 각 레이어의 Perceptron 이 이전 레이어의 모든 Perceptron 과 ***모두*** 연결되어 있음
  - <txtylw>앞</txtylw>과 <txtylw>뒤</txtylw>가 아주 DENSE 하게 연결되어 있음

- `CNN` 에서는 이와 다름

- '머신러닝'에서는 추정하려는 값과 *밀접한 관계* 가 있는 값을 이용하는 것이 중요함

- 여러 개의 `Convolutional Layer` 를 쌓아서 만듦

- Non-linear 한 feature 를 뽑아낼 수 있음
  - 이전 층에서의 특징을 이용해 *high-level feature* 를 뽑을 수 있음

- *Convolution* 이라는 operation 을 사용함
  - *convolution*: 통신, signal processing 등에서 배움

- 전체 정보가 아닌, <txtylw>정해진 일부 영역</txtylw>의 정보를 이용해서 feature 를 뽑아냄

- *같은 층의 feature*는 같은 parameter 를 이용해 추출함
  - feature map 의 위치에 관계 없이 같은 parameter 를 사용함
  - 이를 <txtred>Parameter Sharing</txtred> 라 부름
  - `MLP` 에서는 각 Perceptron 에 대해 <txtred><u>서로 다른</u></txtred> $w$ 를 사용했음을 상기

- *(예) $5\times5$ window 를 전체 영상에 대해 돌며, 각각의 *feature* 를 뽑아내고 이를 *feature map* 으로 만들 수 있음*

---

- `MLP`에 비해 전체 <mark>Parameter</mark> 의 수가 크게 줄어듦
  - $5 \times 5 = 25$ 의 Filter 를 이용,
  - $100 \times 100$ -> $96 \times 96$ (margin 이 빠지기 때문)
  - TL;DR
    - `MLP` 에서는 $10000 \times 9216$ 의 parameter 가 필요하지만
    - `CNN` 에서는 $25$ 개의 parameter 만 필요함.
      - $1000 \times 1000$ 크기가 들어와도 똑같음.
  - Parameter 의 <txtylw>수가 적다</txtylw>는 점이 장점인 이유
    - *계산이 쉬워진다.*
    - Paramter 를 저장할 *메모리* 사이즈도 절약됨
- `CNN` 은 Convolution 을 통과하면서 정보들을 압축하는 정도가 <txtylw>낮음</txtylw>
  - 100 100 -> 96 96 -> 92 92 -> ...
  - 따라서, ***Pooling***과 같은 `Subsampling` Layer 를 추가하여 <txtylw>갯수를 줄이고자 함</txtylw>
    - 예 1/2 로 pooling 하는 layer 가 있다고 하면
    - 100 100 -> 96 96 -> 48 48 -> 44 44 -> 22 22 -> 20 20 -> ...
    - $\therefore$ 메모리 이점 + 정보양 감축
    - <txtred>단점</txtred>: `Subsampling` 을 통해 <txtred>정보 유실</txtred>이 발생
    - <txtylw>보통 parameter 가 없음</txtylw>...
- 마지막에는 `Fully Connected Layer` 로 구성함
  - 적당히 Feature 가 뽑혔으면, ***전체 정보를 다 보고 Classification*** 을 진행함
    - 따라서, Fully Connected Layer 로 구성됨

---

# Convolution
- `continuous signal` 에 대해 정의된 연산
  - ***Input data*** 를 time 축을 기준으로 생각했을 때,
  - Filter 값($w$)과 <txtylw>뒤집힌</txtylw> 형태로 곱해졌음을 이해해야함
    - $\Sigma \\ \big(x\[i - k\] \times w\[k\] \\ \big)$

## 1 차원일 때의 예시

### Stride

<center>

| | | |
|--|--|--|
|$w_1$ |$w_2$ |$w_3$ |
|_ |_ |_ |

<br/>

| | | |
|--|--|--|
|$w_1$ |$w_2$ |$w_3$ |
|$x_1$ |_ |_ |

<br/>

| | | |
|--|--|--|
|$w_1$ |$w_2$ |$w_3$ |
|$x_2$ |$x_1$ |_ |

<br/>

| | | |
|--|--|--|
|$w_1$ |$w_2$ |$w_3$ |
|$x_3$ |$x_2$ |$x_1$ |

</center>

- 이론적으로는 $\Sigma$ 에서 무한대의 범위로 계산됨
- $N$ 칸을 건너뛰며 곱하는 것을 ***Stride*** 라고 함
  - (예) stride = 2 는 두 칸씩 뛰어가며 계산한다는 의미

### Padding
- <txtred>만약</txtred>, Convolution 의 크기가 Filter 보다 줄어들면 진행할 수가 없음.
- $\therefore$ 적절한 padding 을 추가해서 Convolution 이 가능하도록 함.
<br />
<br />
- 줄어드는 정도는 `Kernel(==filter)` 의 크기와 관련있음
  - margin 만큼 감소
- `Same Padding`: 원본 사이즈를 보존하기 위한 padding
  - <txtylw>*줄어드는 만큼</txtylw> padding 을 채워주면 됨*
- `Full Padding`: 원본 사이즈보다 조금 커지는 padding
- 대부분은 <txtred>Same Padding</txtred> 를 사용함

---

# 2 차원 배열에서의 Convolution
- `1차원` 에서와 마찬가지로 $\Sigma \\ \big(x\[i - k\] \times w\[k\] \big)$ 를 위해서
  - $w$ 를 상하, 좌우 반전해줘야 함

---

# Pooling
- 계산량 감소, 의미있는 feature 추출
  - Convolution 을 여러번 하면, 원본의 여러 영역(점점 넓어지는)을 보는 것과 같음
    - 3, 5, 7, ...
  - <txtylw>(+)</txtylw> Pooling 을 통해 얻은 Convolution 의 결과는, 원본에서 보게 되는(값에 영향을 받는) 영역이 크게 넓어짐 (2배)
  - 여기서 바라보는(받아들이는) 영역을 <txtylw>Receptive Field</txtylwui>

