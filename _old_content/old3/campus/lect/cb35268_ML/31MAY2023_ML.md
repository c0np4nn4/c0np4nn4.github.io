+++
title = "ML 31MAY2023"
description = "Deep Learning"
date = 2023-05-31
toc = true

[taxonomies]
categories = ["ML"]
tags = ["Quick Note"]

[extra]
math=true
+++

---
# CNN
- Correlation 연산을 뒤집으면 Convolution 연산?

---
# Convolution w/ Multiple Channels
- `input data` 가 여러 채널로 이루어져 있을 때,  해당 채널의 개수만큼 <txtylw>convolution kernel</txtylw> 을 생성해야 함
- 즉, 각 커널의 크기를 $k_h \times k_w$ 라 할 때, 입력 데이터의 채널 개수 $c_{in}$ 만큼 갖고 있어야 함
$$\therefore k_h \times k_w \times c_{in}$$
- 커널의 채널 수도 맞춰서 convolution 을 진행하면 1 channel 로 줄어든다
- 따라서, $c_{out}$ 개의 Filter 를 갖게 되면 $c_{out}$ 개의 feature 를 뽑을 수 있다고 함.
hadamard product
- <txtred>convolution Layer</txtred> -> <txtred>pooling Layer</txtred>

---
# Receptive Field
Layer 를 거치며 한 field 가 바라보는 영역이 점점 넓어진다는 개념
dilation -> 사이에 여백을 두는 개념 --> 바라보는 영역이 넓어지는 효과
Pooling 을 하나 넣어주면 바라보는 영역이 확 늘어남

---
# Dropout ([ref](https://hyewonleess.github.io/cnn/CNN_options/))
- 모델을 규제하는 기법
  - 규제 == 일반화
  - Hidden Unit 의 일부를 일정 확률로 제거
- 매 번 dropout 되는 뉴런이 달라지기 때문에, 네트워크의 모양이 달라짐
- <txtylw>훈련할 때만 사용</txtylw>, <txtred>Test(Evaluation)할 때는 전부 사용</txtred>
- 마치 여러개의 sub-network 를 Ensemble 한 효과를 얻을 수 있음
> - Ensemble --> Generalization 이 잘 된다...
- 각 뉴런이 전체 네트워크에 일정확률 $p$ 만큼 기여했음을 
- `Inverse Dropout`: 구현 단에서의 효율성

---
# Activation Function
- convolution 을 여러개 쌓으면 결국 하나를 쌓는 것과 같아짐 (linearity 때문)
- 따라서, non-linearity 함수를 중간에 넣는게 필요함
