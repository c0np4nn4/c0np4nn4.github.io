+++
title = "26APR2023"
description = "ML"
date = 2023-04-26
toc = true

[taxonomies]
categories = ["Quick Review"]
tags = ["ML"]

[extra]
math=true
+++

# ML
## Clustering Exercise
- 한 `Cluster` 는 비슷한 샘플로 구성됨
  - ***같은*** `Cluster` 의 샘플들은 유사도가 높음
  - ***다른*** `Cluster` 간의 샘플들은 유사도가 낮음
---
- `sklearn` 을 이용해 아래와 같이 cluster sample 을 만들 수 있음
```python
from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples = 150, # 샘플 수
                  n_features = 2,  # dimension
                  centers = 3,  # k 값, cluster 갯수
                  cluster_std = 0.5,  # cluster 간의 표준편차
                  shuffle = True,
                  random_state = 0
                  )

#  2차원 평면에 그려보기
import matplotlib.pyplot as plt
plt.scatter(X[:, 0], X[:, 1],
            c = 'white', marker='o', edgecolor='black', s=50)
plt.grid()
plt.tight_layout()
plt.show()
```
- 결과는 아래와 같습니다.

<img src="../../images/post/cb35268/clustering_01.png" alt="adsf" width="600rem"/>

- 한 눈에 보기에도$3$개의 cluster 로 나뉠 수 있음을 알 수 있습니다.

---

- `k-means clustering` 은 아래의 과정을 반복하여 clustering 하는 방법입니다.
> - $k$ 개의 *centroid* 를 선택합니다.
> - 각 sample data 를 가장 가까운 *centroid* 에 할당합니다.
> - 각 `cluster` 의 *centroid* 를 구하여 업데이트 합니다.
> - <u>적당히</u> 수렴되었다고 판단되면 종료합니다.
- 이 때, 각 샘플 간의 유사도는 아래 공식을 이용해 구합니다.(Minkowski 연관)
$$d(x, y)^2 = \sum_{j=1}^{m} (x_j - y_j)^2 = || \mathbb{x} - \mathbb{y} ||^2_2$$
- 위 공식에 따라 구해지는 `샘플 간 유사도`를 높이는 방향으로 최적화 해갑니다.
---
- 위 공식을 응용해서 ***각 Cluster 내의 샘플 간 유사도*** 를 구하는 공식을 세우면 아래와 같습니다. (SSE, Sum of squared Errors)
$$\text{SSE} = \sum_{i=1}^{n} \sum_{j=1}^{k} w^{(i, j)} || \mathbb{x}^{(i)} - \mathbb{\mu}^{(j)} ||^{2}_{2}$$
- $w^{(i, j)}$: 샘플 $i$ 의 `Cluster` $j$ 소속 여부
- $\mathbb{\mu}^{(j)}$: `Cluster` $j$ 의 ***Centroid***
- 즉, 각 `Cluster` 에 속하는 샘플들이 해당 `Cluster` 의 중심으로부터 얼마나 떨어져있는지에 대한 총합을 구하는 공식입니다.
- 두 번째 $\sum$ 이 $j=1, \dots, k$ 임에 주목하면 `Cluster` 를 가리키는 값임을 알 수 있습니다.
- `sklearn` 을 이용해 `k-means Clustering` 을 아래 코드와 같이 실시할 수 있습니다.

```python
from sklearn.cluster import KMeans

km = KMeans(n_clusters = 3,  # k = 3
            init = 'random',  # ??
            n_init = 10,  # 10 번 랜덤하게 수행 후 가장 좋은 모델을 선택합니다.
            max_iter = 300,  # 각 시도 당 최대 300 번 계산합니다.
            tol = 1e-04,  # 수렴 여부를 판단하는 기준입니다.
            random_state = 0  # random seed
            )

y_km = km.fit_predict(X)  # X 값은 위에서 생성한 sample blobs 를 이용할 수 있습니다.
```
- 결과를 출력하는 코드를 추가하여 얻은 이미지는 아래와 같습니다.
<img src="../../images/post/cb35268/clustering_02.png" alt="adsf" width="600rem"/>
---
## K-means++ Clustering
- 처음 random $k$ 를 선택할 때, 조금 더 좋은 선택을 하는 방법.
- ***Centroid*** 를 최대한 멀리 떨어지게 하는 방법.
- 확률적으로 선택할 수 있음.
---
## Regularization
- 모델에 `규제`를 더하는 것
  - `SVM` 에서의 *slack variable* 도 해당
- `Linear Model` 에서의 `Regularization`
  - `Subset Selection`: **속성(attr)** 중 쓸모 없는 값들을 제하는 방법
    - 만약 `test set`에서 모든 속성(i.e. 100개)을 사용한다면? -> ***overfitting*** 가능성
    - `Forward`, `Backward` 방식으로 최적의 attr 를 선택하는 방법
  - `Shrinkage`: `Ridge`, `Lasso` 기법
    - `Ridge`: `Coefficient` 의 제곱의 합과 *tuning parameter* 로 불리는 $\lambda$ 값을 곱한 것을 $\text{RSS}$ 와 더함
      - ***Dominant*** 속성의 영향을 줄이고, 좋은 모델을 찾을 수 있다고 함
