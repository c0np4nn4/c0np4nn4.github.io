+++
title = "ML 22MAY2023"
description = "Deep Learning"
date = 2023-05-22
toc = true

[taxonomies]
categories = ["ML"]
tags = ["ML"]

[extra]
math=true
+++

# Review

- `Multi-Layer Perceptron`
  - 특정 레이어의 output 이 다음 레이어의 input 이 됨

- Foward pass 는

- 각 입력에 대해 weight 를 부과
 
- non-linear activation 을 통과 시키면 output 이 됨
 
- 여러 층을 쌓은 이유: ***Non-Linear 문제를 풀기 위해서***


Backward pass

"인공신경망 훈련을 위해서는 Gradient 를 계산해야함"

"Gradient 효율적으로 계산하기 위한 기술이 Backpropagation"

## Back-propagation

*맨 끝의 Loss function 으로 부터 $L$ 레이어까지 어떻게 계산할 수 있는가*?


### Notation
- $w_{jk}^l$: 앞(현재) 레이어 ($l$)의 $j$ 번째 뉴런에서 전 레이어($l-1$)의 $k$ 번째 뉴런에 연결된 가중치
- $b_j^l$: 현재($l$)번째 레이어의 $j$번쨰 뉴런의 bias
- $a_j^l$: 앞 레이어의 출력(activation 의 output)을 기반으로 계산함
  - 앞에 있는 모든 output 들의 $\Sigma$ 와 현재 레이어의 bias 를 더해서 구함
  - non-linearlity 를 위해서 $\delta$ (sigmoid) 를 앞에 곱함 (***Non Linear Activation***)
  - $a_j^l = \delta \big(\Sigma_{k} \big)$
---
- 앞서 정의한 내용을 기반으로, *weight matrix* 를 만들 수 있음
  - dimension: $j \times k$ matrix
- bias 도 마찬가지로 레이어 $l$ 에 대한 bias vector 를 만들 수 있음
  - dimension: $j$
- $\therefore$ activation 도 벡터로 표현할 수 있음
  - $a^{l-1}$ 는 dimension 으로 $k$ 를 가짐 ($k$ 개의 뉴런으로 구성되어 있기 때문)
  - $a^{l} = \delta (w^{l}a^{l-1} + b^{l})$
  - Dimension?: $j\times1 = (j\times k)(k\times 1) + (j\times 1)$
---
- 중간에 나오는 값에 새로운 notation 을 부과
  - $z$: weighted input
  - $z = w^{l}a^{l-1} + b^{l}$
---
### Cost Function
- $C=\frac{1}{n}$
- 각 sample 에 대한 gradient 를 계산하고, 해당 gradient 의 평균값으로 ?
- *Cost function 은 neural network 의 output 으로 계산을 해야함*
- L2 Regulrazation 을 이용해서 계산(오차 제곱의 평균)

---

### Derivation
- 레이어 $L$ 의 $j$번째 뉴런이 ***아주 조금*** 변경되었다고 가정
  - weighted input $z$ 에 일어남
  - $\Delta z_j^{l}$
  - 원래는 $\sigma (z_j^{l})$ 인데, $\sigma (z_j^{l} + \Delta z_j^{l})$
  - Cost 의 변화량 --> 편미분 된 값

---

activation 과 weighted input 사이의관계????

hadamard operation

---
## Remarks
---
- output layer 의 $\delta^{l}$ 를 구하는 방정식.
- 앞 레이어 ($L + 1$) 의 error 를 이용해서 전 레이어 ($L$) 의 error 를 구하는 방정식
  - $\delta^{l} from \delta^{l + 1}$ 를 구하는 방정식.
  - $\delta^{l} = \big((w^{l+1})^T {\delta}^{l+1}\big) \bigodot \sigma'(z^l)$

현재 레이어의 error 는

  (1)현재 레이어의 weighted input 이 다음 레이어의 weighted input 에 기여하는 정도와

  (2)다음 레이어의 weighted input 이 Cost function 에 기여하는 정도를 모두 더함

  근데, (2) 는 다음 레이어의 error 임

(1) 의 분자 값을 잘 살펴보면 $L$ 레이어의 weighted input 으로 치환할 수 있음

$\therefore$ 세 항이 곱해진 형태로 현재 레이어의 에러(${\delta}_j^l$) 를 구할 수 있음

이를 matrix 형태로 수식을 간략히 표현하면 tranposed $W$ 로 표기 되기도 함

---

`bias 의 변화량에 따른 Cost C 의 변화량`

(1) chain rule 로 $z$ 에 대한 변화량으로 변해서 계산할 수 있음

(2) weighted input 에 대한 방정식을 이용함 --> 다 날아가고 1 남음

---

`weight 변화량에 따른 Cost 의 변화량`

bias 때와 똑같은 방식으로 진행함

---

# REF
- [blog post](https://towardsdatascience.com/deriving-the-backpropagation-equations-from-scratch-part-1-343b300c585a)
